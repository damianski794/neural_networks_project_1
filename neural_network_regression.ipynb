{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIECI NEURONOWE, PROJEKT 1\n",
    "#### Autorzy: Mikołaj Rzepiński, Damian Wysokiński"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wybór liczby warstw i liczby węzłów w każdej warstwie\n",
    "\n",
    "Wybór liczby węzłów w warstwie zerowej (input layer) zależy od rodzaju problemu - czy mamy regresję, czy klasyfikację oraz od liczby kolumn z danymi pobieranymi z plików csv. Analogiczna sytuacja jest z ostatnią warstwą (output layer). Ponadto w warstwie ostatniej wybiera się funkcję aktywacji stosowną do problemu:\n",
    "- dla regresji (funkcja liniowa)\n",
    "- dla klasyfikacji (softmax, liczba węzłów zależna od liczby unikalnych wartości w zbiorze uczącym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dla regresji\n",
    "\n",
    "Mamy 2 możliwości:\n",
    "- x - > y\n",
    "- x, y -> z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_problem = True\n",
    "classification_problem = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x           y\n",
      "0 -1.519345 -246.176377\n",
      "1  0.144673 -139.145933\n",
      "2  3.209500  -11.609238\n",
      "3 -1.066817 -223.183756\n",
      "4  1.760798  -44.009923\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.519345</td>\n",
       "      <td>-246.176377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.144673</td>\n",
       "      <td>-139.145933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.209500</td>\n",
       "      <td>-11.609238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.066817</td>\n",
       "      <td>-223.183756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.760798</td>\n",
       "      <td>-44.009923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x           y\n",
       "0 -1.519345 -246.176377\n",
       "1  0.144673 -139.145933\n",
       "2  3.209500  -11.609238\n",
       "3 -1.066817 -223.183756\n",
       "4  1.760798  -44.009923"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dodac dostosowanie do tego czy jest to plik do regresji czy klasyfikacji\n",
    "\n",
    "if(regression_problem):\n",
    "    regression_train_file = 'regression\\data.activation.train.100.csv' #jak działasz na linuxie to musisz dostosować \\ na /\n",
    "    regression_df = pd.read_csv(regression_train_file)\n",
    "    print(regression_df.head())\n",
    "    \n",
    "regression_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(classification_problem):\n",
    "    classification_train_file = 'classification\\data.three_gauss.test.100.csv'\n",
    "    classification_df = pd.read_csv(classification_train_file)\n",
    "    \n",
    "    print(classification_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(classification_problem):\n",
    "    #print(classification_df.shape)\n",
    "    #print(classification_df.tail())\n",
    "    #print(classification_df[['x','y']].to_numpy().T)\n",
    "    x_train = classification_df[['x','y']].to_numpy().T\n",
    "    y_train_normal_labels = classification_df['cls'].to_numpy().reshape(1,-1)\n",
    "    print(y_train_normal_labels, y_train_normal_labels.shape)\n",
    "    no_of_unique_values = np.unique(y_train_normal_labels)\n",
    "    print(no_of_unique_values)\n",
    "    \n",
    "    zeros_array = np.zeros((len(no_of_unique_values),y_train_normal_labels.shape[1]))\n",
    "    final_array = np.zeros(zeros_array.shape)\n",
    "    print(zeros_array)\n",
    "    print(zeros_array.shape)\n",
    "    #print(zeros_array)\n",
    "    for unique_value in no_of_unique_values:\n",
    "        print(unique_value)\n",
    "        #final_array[(unique_value - 1),:] ==\n",
    "        print(y_train_normal_labels[y_train_normal_labels == unique_value])\n",
    "        \n",
    "    print(final_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(regression_problem):\n",
    "    x_train = np.array(regression_df['x']).reshape(1,-1)\n",
    "    y_real = np.array(regression_df['y']).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n_nodes': 1, 'activation_function': None},\n",
       " {'n_nodes': 2, 'activation_function': 'relu'},\n",
       " {'n_nodes': 4, 'activation_function': 'relu'},\n",
       " {'n_nodes': 1, 'activation_function': 'linear'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(regression_problem):\n",
    "    n_nodes_input_layer = 1\n",
    "elif(classification_problem):\n",
    "    n_nodes_input_layer = 2\n",
    "\n",
    "input_layer = [\n",
    "    {\n",
    "        \"n_nodes\": n_nodes_input_layer,\n",
    "        \"activation_function\": None\n",
    "    },\n",
    "]\n",
    "\n",
    "hidden_layers = [\n",
    "    {\n",
    "        \"n_nodes\": 2,\n",
    "        \"activation_function\": \"relu\"\n",
    "    },\n",
    "    {\n",
    "        \"n_nodes\": 4,\n",
    "        \"activation_function\": \"relu\"\n",
    "    },   \n",
    "]\n",
    "\n",
    "if(regression_problem):\n",
    "    n_nodes_output_layer = 1\n",
    "elif(classification_problem):\n",
    "    n_nodes_output_layer = 3\n",
    "    \n",
    "output_layer = [\n",
    "    {\n",
    "        \"n_nodes\": n_nodes_output_layer,\n",
    "        \"activation_function\": \"linear\"\n",
    "    }, ]\n",
    "\n",
    "layers = input_layer + hidden_layers + output_layer\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W zależności od tego czy chcemy uwzględnić bias w sieci można zmieniać wartość include bias jako True/False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_bias = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(x_train, layers, include_bias = True):\n",
    "    # zwraca slownik z wagami i biasami np. parameters['w1'], parameters['b1']\n",
    "    #print(x_train.shape)\n",
    "    #print()\n",
    "    np.random.seed(42) # jeśli chcemy za każdym razem te same wyniki\n",
    "    \n",
    "    n_features = x_train.shape[0]\n",
    "    n_examples = x_train.shape[1]\n",
    "    \n",
    "    n_layers = len(layers) # 1 dla input layer, 1 dla output layer\n",
    "#     print(n_layers)\n",
    "    \n",
    "    parameters = {}\n",
    "    activation_values = {}\n",
    "    \n",
    "    \n",
    "    activation_values['0'] = x_train # wartosci x_train są jednocześnie wartościami aktywacji w zerwowej warstwie\n",
    "    \n",
    "    for n_layer in range(1,n_layers):\n",
    "        #print(n_layer)\n",
    "        #print(layers[n_layer])\n",
    "        parameters[\"W\" + str(n_layer)] = np.random.randn(layers[n_layer][\"n_nodes\"], layers[n_layer-1][\"n_nodes\"]) * 0.01 #wczesniej\n",
    "        if(include_bias):\n",
    "            parameters[\"b\" + str(n_layer)] = np.zeros((layers[n_layer][\"n_nodes\"],1))\n",
    "#         print(parameters[\"W\" + str(n_layer)])\n",
    "     \n",
    "#     print(parameters)\n",
    "    return parameters, activation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, activation_values = initialize_parameters(x_train,layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.00496714],\n",
       "        [-0.00138264]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.00647689,  0.0152303 ],\n",
       "        [-0.00234153, -0.00234137],\n",
       "        [ 0.01579213,  0.00767435],\n",
       "        [-0.00469474,  0.0054256 ]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W3': array([[-0.00463418, -0.0046573 ,  0.00241962, -0.0191328 ]]),\n",
       " 'b3': array([[0.]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': array([[-1.51934453,  0.1446731 ,  3.20949991, -1.06681652,  1.7607981 ,\n",
       "          1.70840608,  4.53553337,  3.0228904 , -1.10129773,  2.52267731,\n",
       "          4.823367  ,  2.84927674,  1.30866658,  4.27079203,  0.68625578,\n",
       "         -0.27886859,  0.2042488 ,  3.01805761,  2.00416261,  0.59999959,\n",
       "          3.80098822,  1.81778992,  4.70265148,  3.66821905,  1.90145948,\n",
       "         -0.34648432,  0.62812005,  1.46123206,  0.92681654,  2.22652618,\n",
       "          0.10110583, -1.52138493, -0.48022763,  1.7374457 ,  4.79876335,\n",
       "          2.83254081,  1.78726461,  3.35024563,  3.98594694,  4.37628621,\n",
       "          4.49568535,  0.81213534, -1.5886341 , -1.02623644,  3.53502416,\n",
       "          2.94771761, -0.31568944,  4.58435169, -1.2957231 ,  0.97205132,\n",
       "          2.37877606,  0.81705418,  4.13108753, -1.30679679,  0.79041549,\n",
       "         -0.98523628,  0.19673326, -1.11164594,  4.12027928, -1.93793277,\n",
       "         -1.15428563,  1.45886217,  2.19511646,  3.26670646,  1.1100335 ,\n",
       "          0.37652113,  0.15608049,  3.32476666, -1.71465448, -1.80027995,\n",
       "         -0.97015125,  3.17983189, -1.51201019,  2.31428388,  0.22145965,\n",
       "         -0.58950835, -0.82484989,  4.88403307,  3.22944994,  2.9858326 ,\n",
       "          3.27069612, -1.72186017, -1.00841581,  3.57095524,  3.04750199,\n",
       "         -0.00557951, -1.92928954,  4.73893034, -0.42615508,  3.54934468,\n",
       "          4.80380419,  0.93350789,  4.31570101,  4.0917049 ,  2.31347538,\n",
       "          4.64612168,  0.53708403,  3.21157187,  4.55099556,  4.49848548]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_forward(parameters, activation_values, z_values,  index_of_layer):\n",
    "    z_values[str(index_of_layer)] = np.dot(parameters['W' + str(index_of_layer)],\n",
    "                                                 activation_values[str(index_of_layer -1)]) + parameters['b' + str(index_of_layer)]\n",
    "#     print(parameters['Z'+str(index_of_layer)])\n",
    "#     print()\n",
    "#     print(parameters['Z'+str(index_of_layer)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_forward(parameters, activation_values, {'xd': 1}, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+ np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return np.maximum(0.1*x, x)\n",
    "\n",
    "def linear(x):\n",
    "    return x;\n",
    "\n",
    "# do zweryfikowania ze wzgledu na obliczanie wzgledem okreslonego wektora(axis)\n",
    "def softmax(x):\n",
    "    expo = np.exp(x)\n",
    "    expo_sum = np.sum(np.exp(x))\n",
    "    return expo/expo_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_with_activation(z, activation_values, activation_function, index):\n",
    "    if(activation_function == 'linear'):\n",
    "        activation_values[str(index)] = linear(z)\n",
    "        \n",
    "    elif(activation_function == 'sigmoid'):\n",
    "        activation_values[str(index)] = sigmoid(z)\n",
    "        \n",
    "    elif(activation_function == 'relu'):\n",
    "        activation_values[str(index)] = relu(z)\n",
    "        \n",
    "    elif(activation_function == 'leaky_relu'):\n",
    "        activation_values[str(index)] = leaky_relu(z)\n",
    "        \n",
    "    elif(activation_function == 'softmax'):\n",
    "        activation_values[str(index)] = softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_functions(layers):\n",
    "    activation_functions = {}\n",
    "    for idx, layer in enumerate(layers):\n",
    "        activation_functions[str(idx)] = layer['activation_function']\n",
    "    \n",
    "    return activation_functions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': None, '1': 'relu', '2': 'relu', '3': 'linear'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_activation_functions(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **activation_values** to słownik zawierający: klucze -> numer warstwy, wartość -> macierz z wartościami aktywacji obliczonymi dla danej warstwy\n",
    "- **activation_functions** to słownik zawierający: klucze -> numer warstwy, wartość -> nazwa funkcji aktywacji dla danej warstwy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_layers_forward_propagation(parameters, activation_values, activation_functions, z_values, no_of_layers):\n",
    "    for idx in range(1, no_of_layers):\n",
    "        z_forward(parameters, activation_values, z_values,  idx)\n",
    "        forward_with_activation(z_values[str(idx)], activation_values, activation_functions[str(idx)], idx)\n",
    "        #print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_derivative():\n",
    "    return 1\n",
    "\n",
    "def relu_derivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def leaky_relu_derivative(x):\n",
    "    x[x<=0] = 0.1\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    #to be implemented\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_layers_back_propagation_hardcoded(y_true, parameters, gradients, activation_values, activation_functions, z_values, no_of_layers):\n",
    "    \n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    # WARSTWA 3\n",
    "    gradients['dZ3'] = (activation_values['3'] - y_true) * 1 # 1 bo to pochodna funkcji liniowej\n",
    "    gradients['dW3'] = 1/m * np.dot(gradients['dZ3'], activation_values['2'].T)\n",
    "    gradients['db3'] = 1/m * np.sum(gradients['dZ3'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 2\n",
    "    gradients['dZ2'] = np.dot(parameters['W3'].T, gradients['dZ3']) * relu_derivative(activation_values['2']) \n",
    "    gradients['dW2'] = 1/m * np.dot(gradients['dZ2'], activation_values['1'].T)\n",
    "    gradients['db2'] = 1/m * np.sum(gradients['dZ2'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 1\n",
    "    gradients['dZ1'] = np.dot(parameters['W2'].T, gradients['dZ2']) * relu_derivative(activation_values['1']) \n",
    "    gradients['dW1'] = 1/m * np.dot(gradients['dZ1'], activation_values['0'].T)\n",
    "    gradients['db1'] = 1/m * np.sum(gradients['dZ1'], axis=1, keepdims=True)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_layer_back_propagation(gradients, activation_values, activation_functions, z_values, error_type, index, y_true):\n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    if(activation_functions[str(index)] == 'linear'):\n",
    "        #print('calling last layer, linear activation', 'index:', index)\n",
    "        activation_function_derivative = linear_derivative()\n",
    "        \n",
    "    elif(activation_functions[str(index)] == 'relu'):\n",
    "        activation_function_derivative = relu_derivative(activation_values[str(index)])\n",
    "        \n",
    "    elif(activation_functions[str(index)] == 'leaky_relu'):\n",
    "        activation_function_derivative = leaky_relu_derivative(activation_values[str(index)])\n",
    "    \n",
    "    if(error_type == 'MSE'):\n",
    "        #print('activation_function_derivative w ostatniej warstwie  - automated')\n",
    "        #print(activation_function_derivative)\n",
    "        \n",
    "        gradients['dZ' + str(index)] = (activation_values[str(index)] - y_true) * activation_function_derivative \n",
    "#     print(\"gradients['dZ3'].shape\", gradients['dZ3'].shape)\n",
    "        gradients['dW' + str(index)] = 1/m * np.dot(gradients['dZ'+str(index)], activation_values[str(index - 1)].T)\n",
    "        gradients['db' + str(index)] = 1/m * np.sum(gradients['dZ'+str(index)], axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid_layer_back_propagation(gradients, activation_values, activation_functions, z_values, error_type, index, y_true):\n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    if(activation_functions[str(index)] == 'linear'):\n",
    "        activation_function_derivative = linear_derivative()\n",
    "        \n",
    "    elif(activation_functions[str(index)] == 'relu'):\n",
    "        #print('calling mid layer, relu activation', 'index:', index)\n",
    "        activation_function_derivative = relu_derivative(activation_values[str(index)])\n",
    "        \n",
    "    elif(activation_functions[str(index)] == 'leaky_relu'):\n",
    "        activation_function_derivative = leaky_relu_derivative(activation_values[str(index)])\n",
    "    \n",
    "     # WARSTWA 1 lub 2\n",
    "        #index = 1 lub 2\n",
    "    if(error_type == 'MSE'):\n",
    "        gradients['dZ' + str(index)] = np.dot(parameters['W'+str(index + 1)].T, gradients['dZ'+str(index+1)]) #* relu_derivative(activation_values[str(index)])#activation_function_derivative\n",
    "#     print(\"gradients['dZ3'].shape\", gradients['dZ3'].shape)\n",
    "        gradients['dW'+str(index)] = 1/m * np.dot(gradients['dZ'+str(index)], activation_values[str(index - 1)].T)\n",
    "        gradients['db'+str(index)] = 1/m * np.sum(gradients['dZ'+str(index)], axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid_layer_back_propagation_vol_two(parameters, gradients, activation_values, activation_functions, z_values, error_type, index, y_true):\n",
    "    # BO NAJWAZNIEJSZE JEST TYLKO TO ZEBY PODAWAC PARAMETERS JAKO ARGUMENT\n",
    "    \n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # WARSTWA 3\n",
    "    #gradients['dZ3'] = (activation_values['3'] - y_true) * 1 # 1 bo to pochodna funkcji liniowej\n",
    "    #gradients['dW3'] = 1/m * np.dot(gradients['dZ3'], activation_values['2'].T)\n",
    "    #gradients['db3'] = 1/m * np.sum(gradients['dZ3'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 2\n",
    "    #gradients['dZ2'] = np.dot(parameters['W3'].T, gradients['dZ3']) * relu_derivative(activation_values['2']) \n",
    "    #gradients['dW2'] = 1/m * np.dot(gradients['dZ2'], activation_values['1'].T)\n",
    "    #gradients['db2'] = 1/m * np.sum(gradients['dZ2'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 1\n",
    "    #gradients['dZ1'] = np.dot(parameters['W2'].T, gradients['dZ2']) * relu_derivative(activation_values['1']) \n",
    "    #gradients['dW1'] = 1/m * np.dot(gradients['dZ1'], activation_values['0'].T)\n",
    "    #gradients['db1'] = 1/m * np.sum(gradients['dZ1'], axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "        # WARSTWA 3\n",
    "    gradients['dZ3'] = (activation_values['3'] - y_true) * 1 # 1 bo to pochodna funkcji liniowej\n",
    "    gradients['dW3'] = 1/m * np.dot(gradients['dZ3'], activation_values['2'].T)\n",
    "    gradients['db3'] = 1/m * np.sum(gradients['dZ3'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 2\n",
    "    gradients['dZ2'] = np.dot(parameters['W3'].T, gradients['dZ3']) * relu_derivative(activation_values['2']) \n",
    "    gradients['dW2'] = 1/m * np.dot(gradients['dZ2'], activation_values['1'].T)\n",
    "    gradients['db2'] = 1/m * np.sum(gradients['dZ2'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 1\n",
    "    gradients['dZ1'] = np.dot(parameters['W2'].T, gradients['dZ2']) * relu_derivative(activation_values['1']) \n",
    "    gradients['dW1'] = 1/m * np.dot(gradients['dZ1'], activation_values['0'].T)\n",
    "    gradients['db1'] = 1/m * np.sum(gradients['dZ1'], axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    #if(error_type == 'MSE'):\n",
    "     #   print('idex w mid layer', index)\n",
    "        \n",
    "     #   gradients['dZ'+str(index)] = np.dot(parameters['W'+str(index + 1)].T, gradients['dZ'+str(index + 1)])  *relu_derivative(activation_values[str(index)])\n",
    "     #   gradients['dW'+str(index)] = 1/m * np.dot(gradients['dZ'+str(index)], activation_values[str(index - 1)].T)\n",
    "     #   gradients['db'+str(index)] = 1/m * np.sum(gradients['dZ'+str(index)], axis=1, keepdims=True)\n",
    "\n",
    "    # w teorii powinno byc to dobrze\n",
    "    \n",
    "    \n",
    "   # gradients['dZ1'] = np.dot(parameters['W2'].T, gradients['dZ2']) * relu_derivative(activation_values['1']) \n",
    "   # gradients['dW1'] = 1/m * np.dot(gradients['dZ1'], activation_values['0'].T)\n",
    "   # gradients['db1'] = 1/m * np.sum(gradients['dZ1'], axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "     # WARSTWA 1\n",
    "    #gradients['dZ1'] = np.dot(parameters['W2'].T, gradients['dZ2']) #* relu_derivative(activation_values['1']) \n",
    "#     print(\"gradients['dZ3'].shape\", gradients['dZ3'].shape)\n",
    "    #gradients['dW1'] = 1/m * np.dot(gradients['dZ1'], activation_values['0'].T)\n",
    "    #gradients['db1'] = 1/m * np.sum(gradients['dZ1'], axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_layers_back_propagation_automated(y_true, parameters, gradients, activation_values, activation_functions, z_values, no_of_layers):\n",
    "    \n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    for i in reversed(range(1,no_of_layers)):\n",
    "        \n",
    "        if(i == no_of_layers - 1):\n",
    "            continue\n",
    "            #last_layer_back_propagation(gradients, activation_values, activation_functions,z_values,'MSE',i,y_true)\n",
    "        \n",
    "        elif(i==2):\n",
    "            #mid_layer_back_propagation_vol_two(gradients,activation_values,activation_functions,z_values,'MSE',i,y_true)\n",
    "            continue\n",
    "           \n",
    "    mid_layer_back_propagation_vol_two(parameters,gradients,activation_values,activation_functions,z_values,'MSE',100,y_true)\n",
    "                                       #gradients, activation_values, activation_functions, z_values, error_type, index, y_true\n",
    "\n",
    "        #else:\n",
    "         #   continue\n",
    "            #mid_layer_back_propagation_vol_two(gradients,activation_values,activation_functions,z_values,'MSE',i,y_true)\n",
    "        \n",
    "    #------------------------------------------------------------------------- \n",
    "    # WARSTWA 3\n",
    "    #gradients['dZ3'] = (activation_values['3'] - y_true) * 1 # 1 bo to pochodna funkcji liniowej\n",
    "    #gradients['dW3'] = 1/m * np.dot(gradients['dZ3'], activation_values['2'].T)\n",
    "    #gradients['db3'] = 1/m * np.sum(gradients['dZ3'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 2\n",
    "   # gradients['dZ2'] = np.dot(parameters['W3'].T, gradients['dZ3']) * relu_derivative(activation_values['2']) \n",
    "   # gradients['dW2'] = 1/m * np.dot(gradients['dZ2'], activation_values['1'].T)\n",
    "   # gradients['db2'] = 1/m * np.sum(gradients['dZ2'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 1\n",
    "   # gradients['dZ1'] = np.dot(parameters['W2'].T, gradients['dZ2']) * relu_derivative(activation_values['1']) \n",
    "   # gradients['dW1'] = 1/m * np.dot(gradients['dZ1'], activation_values['0'].T)\n",
    "   # gradients['db1'] = 1/m * np.sum(gradients['dZ1'], axis=1, keepdims=True)\n",
    "    #----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(y_hat, y_true, error_type = None):\n",
    "#     print('y_hat')\n",
    "#     print(y_hat)\n",
    "#     print('------')\n",
    "#     print('y_true')\n",
    "#     print(y_true)\n",
    "    \n",
    "    n_examples = y_hat.shape[1]\n",
    "    \n",
    "#     print('n_examples:', n_examples)\n",
    "    if(error_type == 'MSE'):\n",
    "#         print('srednie y_true:', 1/n_examples*np.sum(np.abs(y_true)))\n",
    "        \n",
    "#         print('MSE')\n",
    "        #print('MSE value:', 1/n_examples * np.sum((y_true - y_hat)**2))\n",
    "        #print('MSE error: ', 1/n_examples * np.sum((y_true - y_hat)**2))\n",
    "        return 1/n_examples * np.sum((y_true - y_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_activation_values(activation_values):\n",
    "    for key, value in activation_values.items():\n",
    "        print(key)\n",
    "        print(value.shape)\n",
    "        print(value)\n",
    "        print('liczba wartosci wiekszych niz 0', np.sum(value > 1))\n",
    "        print(\"-------\")\n",
    "        \n",
    "        if(key == '3'):\n",
    "            print('xdddddd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, no_of_layers, learning_rate = 0.001):\n",
    "    \n",
    "    for i in range(1,no_of_layers):\n",
    "        parameters['W' + str(i)] += - learning_rate * gradients['dW'+str(i)]\n",
    "        parameters['b' + str(i)] += -learning_rate * gradients['db' + str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_regression(x_train, y_train, layers, no_of_iterations = 5000, include_bias = True, error_type='MSE'):\n",
    "    parameters , activation_values = initialize_parameters(x_train, layers, include_bias)\n",
    "    \n",
    "    g_values = {}\n",
    "    g_prime_values = {}\n",
    "    activation_functions = get_activation_functions(layers) # {'0': 'relu', '1': 'sigmoid', ...}\n",
    "    z_values = {}\n",
    "    gradients = {}\n",
    "    losses = []\n",
    "    \n",
    "    no_of_layers = len(layers)\n",
    "    \n",
    "    for i in range(1, 5000): #5000 \n",
    "        all_layers_forward_propagation(parameters, activation_values, activation_functions, z_values, no_of_layers)\n",
    "        \n",
    "        #if(i%50 == 0):\n",
    "        losses.append(calculate_error(activation_values[str(no_of_layers - 1)], y_train, error_type))\n",
    "        #all_layers_back_propagation_hardcoded(y_train,parameters, gradients, activation_values, activation_functions, z_values, no_of_layers)\n",
    "        \n",
    "        all_layers_back_propagation_automated(y_train,parameters,gradients,activation_values,activation_functions,z_values,no_of_layers)\n",
    "                                           \n",
    "        update_parameters(parameters,gradients,no_of_layers)\n",
    "        \n",
    "    #print(losses)\n",
    "   # print_activation_values(activation_values)\n",
    "    print('ostatni blad po pierwiastkowaniu: ', np.sqrt(losses[-1]))\n",
    "    \n",
    "    #plt.figure(figsize=(20,10))\n",
    "    plt.plot(losses[100:])\n",
    "    #plt.plot(losses)\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ostatni blad po pierwiastkowaniu:  2.1823472075978545\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x_train, layers, no_of_iterations = 5001, include_bias = True\n",
    "new_parameters = neural_network_regression(x_train, y_real, layers, no_of_iterations = 5000, include_bias = include_bias) \n",
    "    #zmienic drugi parametr na y_train gdy będzie znany\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.00496714],\n",
       "        [-0.00138264]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.00647689,  0.0152303 ],\n",
       "        [-0.00234153, -0.00234137],\n",
       "        [ 0.01579213,  0.00767435],\n",
       "        [-0.00469474,  0.0054256 ]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W3': array([[-0.00463418, -0.0046573 ,  0.00241962, -0.0191328 ]]),\n",
       " 'b3': array([[0.]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 1.23709384],\n",
       "        [-1.71293573]]),\n",
       " 'b1': array([[1.14956892],\n",
       "        [2.67216441]]),\n",
       " 'W2': array([[-2.82066400e-01,  7.84714644e-01],\n",
       "        [-2.34153375e-03, -2.34136957e-03],\n",
       "        [ 6.94074494e-01,  8.44468267e-02],\n",
       "        [-1.51320549e+00,  3.06701223e+00]]),\n",
       " 'b2': array([[ 2.19381995],\n",
       "        [ 0.        ],\n",
       "        [-0.11550463],\n",
       "        [ 8.50652144]]),\n",
       " 'W3': array([[-2.32622141e+00, -4.65729754e-03,  7.08448304e-01,\n",
       "         -9.08354606e+00]]),\n",
       " 'b3': array([[-6.13831809]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_regression(parameters, x_test):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if(regression_problem):\n",
    " #   predict_regression(parameters, x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
