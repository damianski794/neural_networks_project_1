{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIECI NEURONOWE, PROJEKT 1\n",
    "#### Autorzy: Mikołaj Rzepiński, Damian Wysokiński"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wybór liczby warstw i liczby węzłów w każdej warstwie\n",
    "\n",
    "Wybór liczby węzłów w warstwie zerowej (input layer) zależy od rodzaju problemu - czy mamy regresję, czy klasyfikację oraz od liczby kolumn z danymi pobieranymi z plików csv. Analogiczna sytuacja jest z ostatnią warstwą (output layer). Ponadto w warstwie ostatniej wybiera się funkcję aktywacji stosowną do problemu:\n",
    "- dla regresji (funkcja liniowa)\n",
    "- dla klasyfikacji (softmax, liczba węzłów zależna od liczby unikalnych wartości w zbiorze uczącym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dla regresji\n",
    "\n",
    "Mamy 2 możliwości:\n",
    "- x - > y\n",
    "- x, y -> z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_problem = True\n",
    "classification_problem = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x           y\n",
      "0 -1.519345 -246.176377\n",
      "1  0.144673 -139.145933\n",
      "2  3.209500  -11.609238\n",
      "3 -1.066817 -223.183756\n",
      "4  1.760798  -44.009923\n"
     ]
    }
   ],
   "source": [
    "#dodac dostosowanie do tego czy jest to plik do regresji czy klasyfikacji\n",
    "\n",
    "if(regression_problem):\n",
    "    regression_train_file = 'regression\\data.activation.train.100.csv' #jak działasz na linuxie to musisz dostosować \\ na /\n",
    "    regression_df = pd.read_csv(regression_train_file)\n",
    "    print(regression_df.head())\n",
    "    \n",
    "#regression_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(classification_problem):\n",
    "    classification_train_file = 'classification\\data.three_gauss.test.100.csv'\n",
    "    #classification_train_file = 'classification\\data.simple.test.10000.csv'\n",
    "    classification_df = pd.read_csv(classification_train_file)\n",
    "    \n",
    "    print(classification_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_softmax_array(values_array):\n",
    "    y_train_normal_labels = values_array.to_numpy().reshape(1,-1)\n",
    "    \n",
    "    unique_values = np.unique(y_train_normal_labels)\n",
    "     \n",
    "    softmax_list = []\n",
    "    for idx, unique_value in enumerate(unique_values):\n",
    "        softmax_list.append(y_train_normal_labels == unique_value)\n",
    "        \n",
    "    return np.array(softmax_list).astype('float').reshape(len(unique_values),y_train_normal_labels.shape[1]).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(classification_problem):\n",
    "    x_train = classification_df[['x','y']].to_numpy().T\n",
    "    y_train = create_softmax_array(classification_df['cls'])\n",
    "    #print('xdxdxdxdxdx')\n",
    "    #print(y_train, y_train.shape)\n",
    "    #print('xdxdxdxdxdd X_TRAIN')\n",
    "    #print(x_train, x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(regression_problem):\n",
    "    x_train = np.array(regression_df['x']).reshape(1,-1)\n",
    "    y_real = np.array(regression_df['y']).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n_nodes': 1, 'activation_function': None},\n",
       " {'n_nodes': 2, 'activation_function': 'relu'},\n",
       " {'n_nodes': 4, 'activation_function': 'relu'},\n",
       " {'n_nodes': 1, 'activation_function': 'linear'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(regression_problem):\n",
    "    n_nodes_input_layer = 1\n",
    "    output_layer_activation_function = \"linear\"\n",
    "elif(classification_problem):\n",
    "    n_nodes_input_layer = 2\n",
    "    output_layer_activation_function = \"softmax\"\n",
    "    \n",
    "\n",
    "input_layer = [\n",
    "    {\n",
    "        \"n_nodes\": n_nodes_input_layer,\n",
    "        \"activation_function\": None\n",
    "    },\n",
    "]\n",
    "\n",
    "hidden_layers = [\n",
    "    {\n",
    "        \"n_nodes\": 2,\n",
    "        \"activation_function\": \"relu\"#\"relu\" \n",
    "    },\n",
    "    {\n",
    "        \"n_nodes\": 4,\n",
    "        \"activation_function\": \"relu\"#\"relu\"\n",
    "    }\n",
    "]\n",
    "\n",
    "if(regression_problem):\n",
    "    n_nodes_output_layer = 1\n",
    "elif(classification_problem):\n",
    "    n_nodes_output_layer = 3\n",
    "    \n",
    "output_layer = [\n",
    "    {\n",
    "        \"n_nodes\": n_nodes_output_layer,\n",
    "        \"activation_function\": output_layer_activation_function\n",
    "    }, ]\n",
    "\n",
    "layers = input_layer + hidden_layers + output_layer\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W zależności od tego czy chcemy uwzględnić bias w sieci można zmieniać wartość include bias jako True/False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_bias = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(x_train, layers, include_bias = True):\n",
    "    # zwraca slownik z wagami i biasami np. parameters['w1'], parameters['b1']\n",
    "    #print(x_train.shape)\n",
    "    #print()\n",
    "    np.random.seed(42) # jeśli chcemy za każdym razem te same wyniki\n",
    "    \n",
    "    n_features = x_train.shape[0]\n",
    "    n_examples = x_train.shape[1]\n",
    "    \n",
    "    n_layers = len(layers) # 1 dla input layer, 1 dla output layer\n",
    "#     print(n_layers)\n",
    "    \n",
    "    parameters = {}\n",
    "    activation_values = {}\n",
    "    \n",
    "    \n",
    "    activation_values['0'] = x_train # wartosci x_train są jednocześnie wartościami aktywacji w zerwowej warstwie\n",
    "    \n",
    "    for n_layer in range(1,n_layers):\n",
    "        #print(n_layer)\n",
    "        #print(layers[n_layer])\n",
    "        parameters[\"W\" + str(n_layer)] = np.random.randn(layers[n_layer][\"n_nodes\"], layers[n_layer-1][\"n_nodes\"]) * 0.01 #wczesniej\n",
    "        if(include_bias):\n",
    "            parameters[\"b\" + str(n_layer)] = np.zeros((layers[n_layer][\"n_nodes\"],1))\n",
    "#         print(parameters[\"W\" + str(n_layer)])\n",
    "     \n",
    "#     print(parameters)\n",
    "    return parameters, activation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, activation_values = initialize_parameters(x_train,layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.00496714],\n",
       "        [-0.00138264]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.00647689,  0.0152303 ],\n",
       "        [-0.00234153, -0.00234137],\n",
       "        [ 0.01579213,  0.00767435],\n",
       "        [-0.00469474,  0.0054256 ]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W3': array([[-0.00463418, -0.0046573 ,  0.00241962, -0.0191328 ]]),\n",
       " 'b3': array([[0.]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': array([[-1.51934453,  0.1446731 ,  3.20949991, -1.06681652,  1.7607981 ,\n",
       "          1.70840608,  4.53553337,  3.0228904 , -1.10129773,  2.52267731,\n",
       "          4.823367  ,  2.84927674,  1.30866658,  4.27079203,  0.68625578,\n",
       "         -0.27886859,  0.2042488 ,  3.01805761,  2.00416261,  0.59999959,\n",
       "          3.80098822,  1.81778992,  4.70265148,  3.66821905,  1.90145948,\n",
       "         -0.34648432,  0.62812005,  1.46123206,  0.92681654,  2.22652618,\n",
       "          0.10110583, -1.52138493, -0.48022763,  1.7374457 ,  4.79876335,\n",
       "          2.83254081,  1.78726461,  3.35024563,  3.98594694,  4.37628621,\n",
       "          4.49568535,  0.81213534, -1.5886341 , -1.02623644,  3.53502416,\n",
       "          2.94771761, -0.31568944,  4.58435169, -1.2957231 ,  0.97205132,\n",
       "          2.37877606,  0.81705418,  4.13108753, -1.30679679,  0.79041549,\n",
       "         -0.98523628,  0.19673326, -1.11164594,  4.12027928, -1.93793277,\n",
       "         -1.15428563,  1.45886217,  2.19511646,  3.26670646,  1.1100335 ,\n",
       "          0.37652113,  0.15608049,  3.32476666, -1.71465448, -1.80027995,\n",
       "         -0.97015125,  3.17983189, -1.51201019,  2.31428388,  0.22145965,\n",
       "         -0.58950835, -0.82484989,  4.88403307,  3.22944994,  2.9858326 ,\n",
       "          3.27069612, -1.72186017, -1.00841581,  3.57095524,  3.04750199,\n",
       "         -0.00557951, -1.92928954,  4.73893034, -0.42615508,  3.54934468,\n",
       "          4.80380419,  0.93350789,  4.31570101,  4.0917049 ,  2.31347538,\n",
       "          4.64612168,  0.53708403,  3.21157187,  4.55099556,  4.49848548]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_forward(parameters, activation_values, z_values,  index_of_layer):\n",
    "    z_values[str(index_of_layer)] = np.dot(parameters['W' + str(index_of_layer)],\n",
    "                                                 activation_values[str(index_of_layer -1)]) + parameters['b' + str(index_of_layer)]\n",
    "#     print(parameters['Z'+str(index_of_layer)])\n",
    "#     print()\n",
    "#     print(parameters['Z'+str(index_of_layer)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_forward(parameters, activation_values, {'xd': 1}, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+ np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return np.maximum(0.1*x, x)\n",
    "\n",
    "def linear(x):\n",
    "    return x;\n",
    "\n",
    "# do zweryfikowania ze wzgledu na obliczanie wzgledem okreslonego wektora(axis)\n",
    "def softmax(x):\n",
    "    expo = np.exp(x)\n",
    "    expo_sum = np.sum(np.exp(x))\n",
    "    return expo/expo_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_with_activation(z, activation_values, activation_function, index):\n",
    "    if(activation_function == 'linear'):\n",
    "        activation_values[str(index)] = linear(z)\n",
    "        \n",
    "    elif(activation_function == 'sigmoid'):\n",
    "        activation_values[str(index)] = sigmoid(z)\n",
    "        \n",
    "    elif(activation_function == 'relu'):\n",
    "        activation_values[str(index)] = relu(z)\n",
    "        \n",
    "    elif(activation_function == 'leaky_relu'):\n",
    "        activation_values[str(index)] = leaky_relu(z)\n",
    "        \n",
    "    elif(activation_function == 'softmax'):\n",
    "        activation_values[str(index)] = softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_functions(layers):\n",
    "    activation_functions = {}\n",
    "    for idx, layer in enumerate(layers):\n",
    "        activation_functions[str(idx)] = layer['activation_function']\n",
    "    \n",
    "    return activation_functions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': None, '1': 'relu', '2': 'relu', '3': 'linear'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_activation_functions(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **activation_values** to słownik zawierający: klucze -> numer warstwy, wartość -> macierz z wartościami aktywacji obliczonymi dla danej warstwy\n",
    "- **activation_functions** to słownik zawierający: klucze -> numer warstwy, wartość -> nazwa funkcji aktywacji dla danej warstwy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_layers_forward_propagation(parameters, activation_values, activation_functions, z_values, no_of_layers):\n",
    "    for idx in range(1, no_of_layers):\n",
    "        z_forward(parameters, activation_values, z_values,  idx)\n",
    "        forward_with_activation(z_values[str(idx)], activation_values, activation_functions[str(idx)], idx)\n",
    "        #print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_derivative():\n",
    "    return 1\n",
    "\n",
    "def relu_derivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def leaky_relu_derivative(x):\n",
    "    x[x<=0] = 0.1\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    #to be implemented\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_layers_back_propagation_hardcoded_regression(y_true, parameters, gradients, activation_values, activation_functions, z_values, no_of_layers):\n",
    "    \n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    # WARSTWA 3\n",
    "    gradients['dZ3'] = (activation_values['3'] - y_true) * 1 # 1 bo to pochodna funkcji liniowej\n",
    "    gradients['dW3'] = 1/m * np.dot(gradients['dZ3'], activation_values['2'].T)\n",
    "    gradients['db3'] = 1/m * np.sum(gradients['dZ3'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 2\n",
    "    gradients['dZ2'] = np.dot(parameters['W3'].T, gradients['dZ3']) * relu_derivative(activation_values['2']) \n",
    "    gradients['dW2'] = 1/m * np.dot(gradients['dZ2'], activation_values['1'].T)\n",
    "    gradients['db2'] = 1/m * np.sum(gradients['dZ2'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 1\n",
    "    gradients['dZ1'] = np.dot(parameters['W2'].T, gradients['dZ2']) * relu_derivative(activation_values['1']) \n",
    "    gradients['dW1'] = 1/m * np.dot(gradients['dZ1'], activation_values['0'].T)\n",
    "    gradients['db1'] = 1/m * np.sum(gradients['dZ1'], axis=1, keepdims=True)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_layer_back_propagation(gradients, activation_values, activation_functions, z_values, error_type, index, y_true):\n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    if(activation_functions[str(index)] == 'linear'):\n",
    "        #print('calling last layer, linear activation', 'index:', index)\n",
    "        activation_function_derivative = linear_derivative()\n",
    "        \n",
    "    elif(activation_functions[str(index)] == 'relu'):\n",
    "        activation_function_derivative = relu_derivative(activation_values[str(index)])\n",
    "        \n",
    "    elif(activation_functions[str(index)] == 'leaky_relu'):\n",
    "        activation_function_derivative = leaky_relu_derivative(activation_values[str(index)])\n",
    "    \n",
    "    if(error_type == 'MSE'):\n",
    "        #print('activation_function_derivative w ostatniej warstwie  - automated')\n",
    "        #print(activation_function_derivative)\n",
    "        \n",
    "        gradients['dZ' + str(index)] = (activation_values[str(index)] - y_true) * activation_function_derivative \n",
    "#     print(\"gradients['dZ3'].shape\", gradients['dZ3'].shape)\n",
    "        gradients['dW' + str(index)] = 1/m * np.dot(gradients['dZ'+str(index)], activation_values[str(index - 1)].T)\n",
    "        gradients['db' + str(index)] = 1/m * np.sum(gradients['dZ'+str(index)], axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid_layer_back_propagation(gradients, activation_values, activation_functions, z_values, error_type, index, y_true):\n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    if(activation_functions[str(index)] == 'linear'):\n",
    "        activation_function_derivative = linear_derivative()\n",
    "        \n",
    "    elif(activation_functions[str(index)] == 'relu'):\n",
    "        #print('calling mid layer, relu activation', 'index:', index)\n",
    "        activation_function_derivative = relu_derivative(activation_values[str(index)])\n",
    "        \n",
    "    elif(activation_functions[str(index)] == 'leaky_relu'):\n",
    "        activation_function_derivative = leaky_relu_derivative(activation_values[str(index)])\n",
    "    \n",
    "     # WARSTWA 1 lub 2\n",
    "        #index = 1 lub 2\n",
    "    if(error_type == 'MSE'):\n",
    "        gradients['dZ' + str(index)] = np.dot(parameters['W'+str(index + 1)].T, gradients['dZ'+str(index+1)]) #* relu_derivative(activation_values[str(index)])#activation_function_derivative\n",
    "#     print(\"gradients['dZ3'].shape\", gradients['dZ3'].shape)\n",
    "        gradients['dW'+str(index)] = 1/m * np.dot(gradients['dZ'+str(index)], activation_values[str(index - 1)].T)\n",
    "        gradients['db'+str(index)] = 1/m * np.sum(gradients['dZ'+str(index)], axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid_layer_back_propagation_vol_two(parameters, gradients, activation_values, activation_functions, z_values, error_type, index, y_true):\n",
    "    # BO NAJWAZNIEJSZE JEST TYLKO TO ZEBY PODAWAC PARAMETERS JAKO ARGUMENT\n",
    "    \n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PONIZEJ POPRAWNE HARDCODED\n",
    "        # WARSTWA 3\n",
    "    #gradients['dZ3'] = (activation_values['3'] - y_true) * 1 # 1 bo to pochodna funkcji liniowej\n",
    "    #gradients['dW3'] = 1/m * np.dot(gradients['dZ3'], activation_values['2'].T)\n",
    "    #gradients['db3'] = 1/m * np.sum(gradients['dZ3'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 2\n",
    "    #gradients['dZ2'] = np.dot(parameters['W3'].T, gradients['dZ3']) * relu_derivative(activation_values['2']) \n",
    "    #gradients['dW2'] = 1/m * np.dot(gradients['dZ2'], activation_values['1'].T)\n",
    "    #gradients['db2'] = 1/m * np.sum(gradients['dZ2'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 1\n",
    "    #gradients['dZ1'] = np.dot(parameters['W2'].T, gradients['dZ2']) * relu_derivative(activation_values['1']) \n",
    "    #gradients['dW1'] = 1/m * np.dot(gradients['dZ1'], activation_values['0'].T)\n",
    "    #gradients['db1'] = 1/m * np.sum(gradients['dZ1'], axis=1, keepdims=True)\n",
    "    #POWYZEJ POPRAWNE HARDCODED\n",
    "    \n",
    "    if(activation_functions[str(index)] == 'linear'):\n",
    "        #print('calling last layer, linear activation', 'index:', index)\n",
    "        activation_function_derivative = linear_derivative()\n",
    "        \n",
    "    elif(activation_functions[str(index)] == 'relu'):\n",
    "        activation_function_derivative = relu_derivative(activation_values[str(index)])\n",
    "        \n",
    "    elif(activation_functions[str(index)] == 'leaky_relu'):\n",
    "        #activation_function_derivative = leaky_relu_derivative(activation_values[str(index)])\n",
    "        activation_function_derivative = leaky_relu_derivative(activation_values[str(index)])\n",
    "    \n",
    "    elif(activation_functions[str(index)] == 'sigmoid'):\n",
    "        activation_function_derivative = sigmoid_derivative(activation_values[str(index)])\n",
    "        \n",
    "    \n",
    "    if(error_type == 'MSE'):\n",
    "        #print('idex w mid layer', index)    \n",
    "        gradients['dZ'+str(index)] = np.dot(parameters['W'+str(index + 1)].T, gradients['dZ'+str(index + 1)])  * activation_function_derivative#relu_derivative(activation_values[str(index)])\n",
    "    \n",
    "    gradients['dW'+str(index)] = 1/m * np.dot(gradients['dZ'+str(index)], activation_values[str(index - 1)].T)\n",
    "    gradients['db'+str(index)] = 1/m * np.sum(gradients['dZ'+str(index)], axis=1, keepdims=True)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_layers_back_propagation_automated(y_true, parameters, gradients, activation_values, activation_functions, z_values, no_of_layers):\n",
    "    \n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    for i in reversed(range(1,no_of_layers)):\n",
    "        \n",
    "        if(i == no_of_layers - 1):\n",
    "            \n",
    "            last_layer_back_propagation(gradients, activation_values, activation_functions,z_values,'MSE',i,y_true)\n",
    "        \n",
    "        else:\n",
    "            mid_layer_back_propagation_vol_two(parameters,gradients,activation_values,activation_functions,z_values,'MSE',i,y_true)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(y_hat, y_true, error_type = None):\n",
    "    \n",
    "    n_examples = y_hat.shape[1]\n",
    "    \n",
    "    if(error_type == 'MSE'):\n",
    "        return 1/n_examples * np.sum((y_true - y_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_activation_values(activation_values):\n",
    "    for key, value in activation_values.items():\n",
    "        print(key)\n",
    "        print(value.shape)\n",
    "        print(value)\n",
    "        print('liczba wartosci wiekszych niz 0', np.sum(value > 1))\n",
    "        print(\"-------\")\n",
    "        \n",
    "        if(key == '3'):\n",
    "            print('xdddddd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, no_of_layers, learning_rate = 0.001):#0.001\n",
    "    \n",
    "    for i in range(1,no_of_layers):\n",
    "        parameters['W' + str(i)] -= learning_rate * gradients['dW'+str(i)]\n",
    "        parameters['b' + str(i)] -= learning_rate * gradients['db' + str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_regression(x_train, y_train, layers, no_of_iterations = 5000, include_bias = True, error_type='MSE'):\n",
    "    parameters , activation_values = initialize_parameters(x_train, layers, include_bias)\n",
    "    \n",
    "    g_values = {}\n",
    "    g_prime_values = {}\n",
    "    activation_functions = get_activation_functions(layers) # {'0': 'relu', '1': 'sigmoid', ...}\n",
    "    z_values = {}\n",
    "    gradients = {}\n",
    "    losses = []\n",
    "    \n",
    "    no_of_layers = len(layers)\n",
    "    \n",
    "    for i in range(1, no_of_iterations): #5000 \n",
    "        all_layers_forward_propagation(parameters, activation_values, activation_functions, z_values, no_of_layers)\n",
    "        \n",
    "        #if(i%50 == 0):\n",
    "        losses.append(calculate_error(activation_values[str(no_of_layers - 1)], y_train, error_type))\n",
    "        #all_layers_back_propagation_hardcoded_hardcoded(y_train,parameters, gradients, activation_values, activation_functions, z_values, no_of_layers)\n",
    "        \n",
    "        all_layers_back_propagation_automated(y_train,parameters,gradients,activation_values,activation_functions,z_values,no_of_layers)\n",
    "                                           \n",
    "        update_parameters(parameters,gradients,no_of_layers)\n",
    "        \n",
    "    #print(losses)\n",
    "   # print_activation_values(activation_values)\n",
    "    print('ostatni blad po pierwiastkowaniu: ', np.sqrt(losses[-1]))\n",
    "    \n",
    "    #plt.figure(figsize=(20,10))\n",
    "    plt.plot(losses[:])\n",
    "    #plt.plot(losses)\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ostatni blad po pierwiastkowaniu:  2.1823472075978545\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x_train, layers, no_of_iterations = 5001, include_bias = True\n",
    "new_parameters = neural_network_regression(x_train, y_real, layers, no_of_iterations = 5000, include_bias = include_bias) \n",
    "    #zmienic drugi parametr na y_train gdy będzie znany\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.00496714],\n",
       "        [-0.00138264]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.00647689,  0.0152303 ],\n",
       "        [-0.00234153, -0.00234137],\n",
       "        [ 0.01579213,  0.00767435],\n",
       "        [-0.00469474,  0.0054256 ]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W3': array([[-0.00463418, -0.0046573 ,  0.00241962, -0.0191328 ]]),\n",
       " 'b3': array([[0.]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 1.23709384],\n",
       "        [-1.71293573]]),\n",
       " 'b1': array([[1.14956892],\n",
       "        [2.67216441]]),\n",
       " 'W2': array([[-2.82066400e-01,  7.84714644e-01],\n",
       "        [-2.34153375e-03, -2.34136957e-03],\n",
       "        [ 6.94074494e-01,  8.44468267e-02],\n",
       "        [-1.51320549e+00,  3.06701223e+00]]),\n",
       " 'b2': array([[ 2.19381995],\n",
       "        [ 0.        ],\n",
       "        [-0.11550463],\n",
       "        [ 8.50652144]]),\n",
       " 'W3': array([[-2.32622141e+00, -4.65729754e-03,  7.08448304e-01,\n",
       "         -9.08354606e+00]]),\n",
       " 'b3': array([[-6.13831809]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_regression(parameters, x_test):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if(regression_problem):\n",
    " #   predict_regression(parameters, x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
