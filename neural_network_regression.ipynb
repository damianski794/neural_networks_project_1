{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIECI NEURONOWE, PROJEKT 1\n",
    "#### Autorzy: Mikołaj Rzepiński, Damian Wysokiński"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wybór liczby warstw i liczby węzłów w każdej warstwie\n",
    "\n",
    "Wybór liczby węzłów w warstwie zerowej (input layer) zależy od rodzaju problemu - czy mamy regresję, czy klasyfikację oraz od liczby kolumn z danymi pobieranymi z plików csv. Analogiczna sytuacja jest z ostatnią warstwą (output layer). Ponadto w warstwie ostatniej wybiera się funkcję aktywacji stosowną do problemu:\n",
    "- dla regresji (funkcja liniowa)\n",
    "- dla klasyfikacji (softmax, liczba węzłów zależna od liczby unikalnych wartości w zbiorze uczącym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dla regresji\n",
    "\n",
    "Mamy 2 możliwości:\n",
    "- x - > y\n",
    "- x, y -> z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_problem = True\n",
    "classification_problem = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x           y\n",
      "0 -1.519345 -246.176377\n",
      "1  0.144673 -139.145933\n",
      "2  3.209500  -11.609238\n",
      "3 -1.066817 -223.183756\n",
      "4  1.760798  -44.009923\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.519345</td>\n",
       "      <td>-246.176377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.144673</td>\n",
       "      <td>-139.145933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.209500</td>\n",
       "      <td>-11.609238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.066817</td>\n",
       "      <td>-223.183756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.760798</td>\n",
       "      <td>-44.009923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x           y\n",
       "0 -1.519345 -246.176377\n",
       "1  0.144673 -139.145933\n",
       "2  3.209500  -11.609238\n",
       "3 -1.066817 -223.183756\n",
       "4  1.760798  -44.009923"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dodac dostosowanie do tego czy jest to plik do regresji czy klasyfikacji\n",
    "\n",
    "if(regression_problem):\n",
    "    regression_train_file = 'regression\\data.activation.train.100.csv' #jak działasz na linuxie to musisz dostosować \\ na /\n",
    "    regression_df = pd.read_csv(regression_train_file)\n",
    "    print(regression_df.head())\n",
    "    \n",
    "regression_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(regression_problem):\n",
    "    x_train = np.array(regression_df['x']).reshape(1,-1)\n",
    "    y_real = np.array(regression_df['y']).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n_nodes': 1, 'activation_function': None},\n",
       " {'n_nodes': 2, 'activation_function': 'relu'},\n",
       " {'n_nodes': 4, 'activation_function': 'relu'},\n",
       " {'n_nodes': 1, 'activation_function': 'linear'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(regression_problem):\n",
    "    n_nodes_input_layer = 1\n",
    "elif(classification_problem):\n",
    "    n_nodes_input_layer = 2\n",
    "\n",
    "input_layer = [\n",
    "    {\n",
    "        \"n_nodes\": n_nodes_input_layer,\n",
    "        \"activation_function\": None\n",
    "    },\n",
    "]\n",
    "\n",
    "hidden_layers = [\n",
    "    {\n",
    "        \"n_nodes\": 2,\n",
    "        \"activation_function\": \"relu\"\n",
    "    },\n",
    "    {\n",
    "        \"n_nodes\": 4,\n",
    "        \"activation_function\": \"relu\"\n",
    "    },   \n",
    "]\n",
    "\n",
    "if(regression_problem):\n",
    "    n_nodes_output_layer = 1\n",
    "elif(classification_problem):\n",
    "    n_nodes_output_layer = 3\n",
    "    \n",
    "output_layer = [\n",
    "    {\n",
    "        \"n_nodes\": n_nodes_output_layer,\n",
    "        \"activation_function\": \"linear\"\n",
    "    }, ]\n",
    "\n",
    "layers = input_layer + hidden_layers + output_layer\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W zależności od tego czy chcemy uwzględnić bias w sieci można zmieniać wartość include bias jako True/False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_bias = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(x_train, layers, include_bias = True):\n",
    "    # zwraca slownik z wagami i biasami np. parameters['w1'], parameters['b1']\n",
    "    #print(x_train.shape)\n",
    "    #print()\n",
    "    np.random.seed(42) # jeśli chcemy za każdym razem te same wyniki\n",
    "    \n",
    "    n_features = x_train.shape[0]\n",
    "    n_examples = x_train.shape[1]\n",
    "    \n",
    "    n_layers = len(layers) # 1 dla input layer, 1 dla output layer\n",
    "#     print(n_layers)\n",
    "    \n",
    "    parameters = {}\n",
    "    activation_values = {}\n",
    "    \n",
    "    \n",
    "    activation_values['0'] = x_train # wartosci x_train są jednocześnie wartościami aktywacji w zerwowej warstwie\n",
    "    \n",
    "    for n_layer in range(1,n_layers):\n",
    "        #print(n_layer)\n",
    "        #print(layers[n_layer])\n",
    "        parameters[\"W\" + str(n_layer)] = np.random.randn(layers[n_layer][\"n_nodes\"], layers[n_layer-1][\"n_nodes\"]) * 0.01 #wczesniej\n",
    "        if(include_bias):\n",
    "            parameters[\"b\" + str(n_layer)] = np.zeros((layers[n_layer][\"n_nodes\"],1))\n",
    "#         print(parameters[\"W\" + str(n_layer)])\n",
    "     \n",
    "#     print(parameters)\n",
    "    return parameters, activation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, activation_values = initialize_parameters(x_train,layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.00496714],\n",
       "        [-0.00138264]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.00647689,  0.0152303 ],\n",
       "        [-0.00234153, -0.00234137],\n",
       "        [ 0.01579213,  0.00767435],\n",
       "        [-0.00469474,  0.0054256 ]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W3': array([[-0.00463418, -0.0046573 ,  0.00241962, -0.0191328 ]]),\n",
       " 'b3': array([[0.]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': array([[-1.51934453,  0.1446731 ,  3.20949991, -1.06681652,  1.7607981 ,\n",
       "          1.70840608,  4.53553337,  3.0228904 , -1.10129773,  2.52267731,\n",
       "          4.823367  ,  2.84927674,  1.30866658,  4.27079203,  0.68625578,\n",
       "         -0.27886859,  0.2042488 ,  3.01805761,  2.00416261,  0.59999959,\n",
       "          3.80098822,  1.81778992,  4.70265148,  3.66821905,  1.90145948,\n",
       "         -0.34648432,  0.62812005,  1.46123206,  0.92681654,  2.22652618,\n",
       "          0.10110583, -1.52138493, -0.48022763,  1.7374457 ,  4.79876335,\n",
       "          2.83254081,  1.78726461,  3.35024563,  3.98594694,  4.37628621,\n",
       "          4.49568535,  0.81213534, -1.5886341 , -1.02623644,  3.53502416,\n",
       "          2.94771761, -0.31568944,  4.58435169, -1.2957231 ,  0.97205132,\n",
       "          2.37877606,  0.81705418,  4.13108753, -1.30679679,  0.79041549,\n",
       "         -0.98523628,  0.19673326, -1.11164594,  4.12027928, -1.93793277,\n",
       "         -1.15428563,  1.45886217,  2.19511646,  3.26670646,  1.1100335 ,\n",
       "          0.37652113,  0.15608049,  3.32476666, -1.71465448, -1.80027995,\n",
       "         -0.97015125,  3.17983189, -1.51201019,  2.31428388,  0.22145965,\n",
       "         -0.58950835, -0.82484989,  4.88403307,  3.22944994,  2.9858326 ,\n",
       "          3.27069612, -1.72186017, -1.00841581,  3.57095524,  3.04750199,\n",
       "         -0.00557951, -1.92928954,  4.73893034, -0.42615508,  3.54934468,\n",
       "          4.80380419,  0.93350789,  4.31570101,  4.0917049 ,  2.31347538,\n",
       "          4.64612168,  0.53708403,  3.21157187,  4.55099556,  4.49848548]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_forward(parameters, activation_values, z_values,  index_of_layer):\n",
    "    z_values[str(index_of_layer)] = np.dot(parameters['W' + str(index_of_layer)],\n",
    "                                                 activation_values[str(index_of_layer -1)]) + parameters['b' + str(index_of_layer)]\n",
    "#     print(parameters['Z'+str(index_of_layer)])\n",
    "#     print()\n",
    "#     print(parameters['Z'+str(index_of_layer)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_forward(parameters, activation_values, {'xd': 1}, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+ np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return np.maximum(0.1*x, x)\n",
    "\n",
    "def linear(x):\n",
    "    return x;\n",
    "\n",
    "# do zweryfikowania ze wzgledu na obliczanie wzgledem okreslonego wektora(axis)\n",
    "def softmax(x):\n",
    "    expo = np.exp(x)\n",
    "    expo_sum = np.sum(np.exp(x))\n",
    "    return expo/expo_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_with_activation(z, activation_values, activation_function, index):\n",
    "    if(activation_function == 'linear'):\n",
    "        activation_values[str(index)] = linear(z)\n",
    "        \n",
    "    elif(activation_function == 'sigmoid'):\n",
    "        activation_values[str(index)] = sigmoid(z)\n",
    "        \n",
    "    elif(activation_function == 'relu'):\n",
    "        activation_values[str(index)] = relu(z)\n",
    "        \n",
    "    elif(activation_function == 'leaky_relu'):\n",
    "        activation_values[str(index)] = leaky_relu(z)\n",
    "        \n",
    "    elif(activation_function == 'softmax'):\n",
    "        activation_values[str(index)] = softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_functions(layers):\n",
    "    activation_functions = {}\n",
    "    for idx, layer in enumerate(layers):\n",
    "        activation_functions[str(idx)] = layer['activation_function']\n",
    "    \n",
    "    return activation_functions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': None, '1': 'relu', '2': 'relu', '3': 'linear'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_activation_functions(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **activation_values** to słownik zawierający: klucze -> numer warstwy, wartość -> macierz z wartościami aktywacji obliczonymi dla danej warstwy\n",
    "- **activation_functions** to słownik zawierający: klucze -> numer warstwy, wartość -> nazwa funkcji aktywacji dla danej warstwy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_layers_forward_propagation(parameters, activation_values, activation_functions, z_values, no_of_layers):\n",
    "    for idx in range(1, no_of_layers):\n",
    "        z_forward(parameters, activation_values, z_values,  idx)\n",
    "        forward_with_activation(z_values[str(idx)], activation_values, activation_functions[str(idx)], idx)\n",
    "        #print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_derivative():\n",
    "    return 1\n",
    "\n",
    "def relu_derivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def leaky_relu_derivative(x):\n",
    "    x[x<=0] = 0.1\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    #to be implemented\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_layers_back_propagation_hardcoded(y_true, parameters, gradients, activation_values, activation_functions, z_values, no_of_layers):\n",
    "    \n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    # WARSTWA 3\n",
    "    gradients['dZ3'] = (activation_values['3'] - y_true) * 1 # 1 bo to pochodna funkcji liniowej\n",
    "    gradients['dW3'] = 1/m * np.dot(gradients['dZ3'], activation_values['2'].T)\n",
    "    gradients['db3'] = 1/m * np.sum(gradients['dZ3'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 2\n",
    "    gradients['dZ2'] = np.dot(parameters['W3'].T, gradients['dZ3']) * relu_derivative(activation_values['2']) \n",
    "    gradients['dW2'] = 1/m * np.dot(gradients['dZ2'], activation_values['1'].T)\n",
    "    gradients['db2'] = 1/m * np.sum(gradients['dZ2'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 1\n",
    "    gradients['dZ1'] = np.dot(parameters['W2'].T, gradients['dZ2']) * relu_derivative(activation_values['1']) \n",
    "    gradients['dW1'] = 1/m * np.dot(gradients['dZ1'], activation_values['0'].T)\n",
    "    gradients['db1'] = 1/m * np.sum(gradients['dZ1'], axis=1, keepdims=True)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_layer_back_propagation(gradients, activation_values, activation_functions, z_values, error_type, index, y_true):\n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    if(activation_functions[str(index)] == 'linear'):\n",
    "        #print('calling last layer, linear activation', 'index:', index)\n",
    "        activation_function_derivative = linear_derivative()\n",
    "        \n",
    "    elif(activation_functions[str(index)] == 'relu'):\n",
    "        activation_function_derivative = relu_derivative(activation_values[str(index)])\n",
    "        \n",
    "    elif(activation_functions[str(index)] == 'leaky_relu'):\n",
    "        activation_function_derivative = leaky_relu_derivative(activation_values[str(index)])\n",
    "    \n",
    "    if(error_type == 'MSE'):\n",
    "        #print('activation_function_derivative w ostatniej warstwie  - automated')\n",
    "        #print(activation_function_derivative)\n",
    "        \n",
    "        gradients['dZ' + str(index)] = (activation_values[str(index)] - y_true) * activation_function_derivative \n",
    "#     print(\"gradients['dZ3'].shape\", gradients['dZ3'].shape)\n",
    "        gradients['dW' + str(index)] = 1/m * np.dot(gradients['dZ'+str(index)], activation_values[str(index - 1)].T)\n",
    "        gradients['db' + str(index)] = 1/m * np.sum(gradients['dZ'+str(index)], axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid_layer_back_propagation(gradients, activation_values, activation_functions, z_values, error_type, index, y_true):\n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    if(activation_functions[str(index)] == 'linear'):\n",
    "        activation_function_derivative = linear_derivative()\n",
    "        \n",
    "    elif(activation_functions[str(index)] == 'relu'):\n",
    "        #print('calling mid layer, relu activation', 'index:', index)\n",
    "        activation_function_derivative = relu_derivative(activation_values[str(index)])\n",
    "        \n",
    "    elif(activation_functions[str(index)] == 'leaky_relu'):\n",
    "        activation_function_derivative = leaky_relu_derivative(activation_values[str(index)])\n",
    "    \n",
    "     # WARSTWA 1 lub 2\n",
    "        #index = 1 lub 2\n",
    "    if(error_type == 'MSE'):\n",
    "        gradients['dZ' + str(index)] = np.dot(parameters['W'+str(index + 1)].T, gradients['dZ'+str(index+1)]) #* relu_derivative(activation_values[str(index)])#activation_function_derivative\n",
    "#     print(\"gradients['dZ3'].shape\", gradients['dZ3'].shape)\n",
    "        gradients['dW'+str(index)] = 1/m * np.dot(gradients['dZ'+str(index)], activation_values[str(index - 1)].T)\n",
    "        gradients['db'+str(index)] = 1/m * np.sum(gradients['dZ'+str(index)], axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid_layer_back_propagation_vol_two(gradients, activation_values, activation_functions, z_values, error_type, index, y_true):\n",
    "    \n",
    "    \n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    \n",
    "    if(error_type == 'MSE'):\n",
    "        gradients['dZ'+str(index)] = np.dot(parameters['W'+str(index + 1)].T, gradients['dZ'+str(index + 1)])  *relu_derivative(activation_values[str(index)])\n",
    "        gradients['dW'+str(index)] = 1/m * np.dot(gradients['dZ'+str(index)], activation_values[str(index - 1)].T)\n",
    "        gradients['db'+str(index)] = 1/m * np.sum(gradients['dZ'+str(index)], axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "   # gradients['dZ1'] = np.dot(parameters['W2'].T, gradients['dZ2']) * relu_derivative(activation_values['1']) \n",
    "   # gradients['dW1'] = 1/m * np.dot(gradients['dZ1'], activation_values['0'].T)\n",
    "   # gradients['db1'] = 1/m * np.sum(gradients['dZ1'], axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "     # WARSTWA 1\n",
    "    #gradients['dZ1'] = np.dot(parameters['W2'].T, gradients['dZ2']) #* relu_derivative(activation_values['1']) \n",
    "#     print(\"gradients['dZ3'].shape\", gradients['dZ3'].shape)\n",
    "    #gradients['dW1'] = 1/m * np.dot(gradients['dZ1'], activation_values['0'].T)\n",
    "    #gradients['db1'] = 1/m * np.sum(gradients['dZ1'], axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_layers_back_propagation_automated(y_true, parameters, gradients, activation_values, activation_functions, z_values, no_of_layers):\n",
    "    \n",
    "    m = y_true.shape[1]\n",
    "    \n",
    "    for i in reversed(range(1,no_of_layers)):\n",
    "        \n",
    "        if(i == no_of_layers - 1):\n",
    "            last_layer_back_propagation(gradients, activation_values, activation_functions,z_values,'MSE',i,y_true)\n",
    "        \n",
    "        else:\n",
    "            mid_layer_back_propagation_vol_two(gradients,activation_values,activation_functions,z_values,'MSE',i,y_true)\n",
    "        \n",
    "    \n",
    "            \n",
    "            \n",
    "            # WARSTWA 1\n",
    "    #gradients['dZ1'] = np.dot(parameters['W2'].T, gradients['dZ2']) * relu_derivative(activation_values['1']) \n",
    "#     print(\"gradients['dZ3'].shape\", gradients['dZ3'].shape)\n",
    "    #gradients['dW1'] = 1/m * np.dot(gradients['dZ1'], activation_values['0'].T)\n",
    "    #gradients['db1'] = 1/m * np.sum(gradients['dZ1'], axis=1, keepdims=True)\n",
    "\n",
    "            \n",
    "            \n",
    "#     gradients['dZ2'] = np.dot(parameters['W3'].T, gradients['dZ3']) * relu_derivative(z_values['2'])\n",
    "#     print(\"gradients['dZ3'].shape\", gradients['dZ3'].shape)\n",
    "#     gradients['dW2'] = 1/m * np.dot(gradients['dZ2'], activation_values['1'].T)\n",
    "#     gradients['db2'] = 1/m * np.sum(gradients['dZ2'], axis=1, keepdims=True)\n",
    "    \n",
    "    # WARSTWA 1\n",
    "#     gradients['dZ1'] = np.dot(parameters['W2'].T, gradients['dZ2']) * relu_derivative(z_values['1'])\n",
    "#     print(\"gradients['dZ3'].shape\", gradients['dZ3'].shape)\n",
    "#     gradients['dW1'] = 1/m * np.dot(gradients['dZ1'], activation_values['0'].T)\n",
    "#     gradients['db1'] = 1/m * np.sum(gradients['dZ1'], axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(y_hat, y_true, error_type = None):\n",
    "#     print('y_hat')\n",
    "#     print(y_hat)\n",
    "#     print('------')\n",
    "#     print('y_true')\n",
    "#     print(y_true)\n",
    "    \n",
    "    n_examples = y_hat.shape[1]\n",
    "    \n",
    "#     print('n_examples:', n_examples)\n",
    "    if(error_type == 'MSE'):\n",
    "#         print('srednie y_true:', 1/n_examples*np.sum(np.abs(y_true)))\n",
    "        \n",
    "#         print('MSE')\n",
    "        #print('MSE value:', 1/n_examples * np.sum((y_true - y_hat)**2))\n",
    "        #print('MSE error: ', 1/n_examples * np.sum((y_true - y_hat)**2))\n",
    "        return 1/n_examples * np.sum((y_true - y_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_activation_values(activation_values):\n",
    "    for key, value in activation_values.items():\n",
    "        print(key)\n",
    "        print(value.shape)\n",
    "        print(value)\n",
    "        print('liczba wartosci wiekszych niz 0', np.sum(value > 1))\n",
    "        print(\"-------\")\n",
    "        \n",
    "        if(key == '3'):\n",
    "            print('xdddddd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, no_of_layers, learning_rate = 0.001):\n",
    "    \n",
    "    for i in range(1,no_of_layers):\n",
    "        parameters['W' + str(i)] += - learning_rate * gradients['dW'+str(i)]\n",
    "        parameters['b' + str(i)] += -learning_rate * gradients['db' + str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_regression(x_train, y_train, layers, no_of_iterations = 5000, include_bias = True, error_type='MSE'):\n",
    "    parameters , activation_values = initialize_parameters(x_train, layers, include_bias)\n",
    "    \n",
    "    g_values = {}\n",
    "    g_prime_values = {}\n",
    "    activation_functions = get_activation_functions(layers) # {'0': 'relu', '1': 'sigmoid', ...}\n",
    "    z_values = {}\n",
    "    gradients = {}\n",
    "    losses = []\n",
    "    \n",
    "    no_of_layers = len(layers)\n",
    "    \n",
    "    for i in range(1, 5000): \n",
    "        all_layers_forward_propagation(parameters, activation_values, activation_functions, z_values, no_of_layers)\n",
    "        \n",
    "        #if(i%50 == 0):\n",
    "        losses.append(calculate_error(activation_values[str(no_of_layers - 1)], y_train, error_type))\n",
    "        #all_layers_back_propagation_hardcoded(y_train,parameters, gradients, activation_values, activation_functions, z_values, no_of_layers)\n",
    "        all_layers_back_propagation_automated(y_train,parameters,gradients,activation_values,activation_functions,z_values,no_of_layers)\n",
    "        update_parameters(parameters,gradients,no_of_layers)\n",
    "        \n",
    "    #print(losses)\n",
    "   # print_activation_values(activation_values)\n",
    "    print('ostatni blad po pierwiastkowaniu: ', np.sqrt(losses[-1]))\n",
    "    \n",
    "    #plt.figure(figsize=(20,10))\n",
    "    plt.plot(losses[100:])\n",
    "    #plt.plot(losses)\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ostatni blad po pierwiastkowaniu:  88.57438918842027\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x_train, layers, no_of_iterations = 5001, include_bias = True\n",
    "new_parameters = neural_network_regression(x_train, y_real, layers, no_of_iterations = 5000, include_bias = include_bias) \n",
    "#zmienic drugi parametr na y_train gdy będzie znany\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.00496714],\n",
       "        [-0.00138264]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.00647689,  0.0152303 ],\n",
       "        [-0.00234153, -0.00234137],\n",
       "        [ 0.01579213,  0.00767435],\n",
       "        [-0.00469474,  0.0054256 ]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W3': array([[-0.00463418, -0.0046573 ,  0.00241962, -0.0191328 ]]),\n",
       " 'b3': array([[0.]])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 1.23709384],\n",
       "        [-1.71293573]]),\n",
       " 'b1': array([[1.14956892],\n",
       "        [2.67216441]]),\n",
       " 'W2': array([[-2.82066400e-01,  7.84714644e-01],\n",
       "        [-2.34153375e-03, -2.34136957e-03],\n",
       "        [ 6.94074494e-01,  8.44468267e-02],\n",
       "        [-1.51320549e+00,  3.06701223e+00]]),\n",
       " 'b2': array([[ 2.19381995],\n",
       "        [ 0.        ],\n",
       "        [-0.11550463],\n",
       "        [ 8.50652144]]),\n",
       " 'W3': array([[-2.32622141e+00, -4.65729754e-03,  7.08448304e-01,\n",
       "         -9.08354606e+00]]),\n",
       " 'b3': array([[-6.13831809]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_regression(parameters, x_test):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if(regression_problem):\n",
    " #   predict_regression(parameters, x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
